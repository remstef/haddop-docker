
networks:
  hadoopnet:
    driver: overlay

services:

  headnode:
    image: remstef/hadoop3-jobimtext
    hostname: headnode
    command: >-
      /bin/bash -c "
        ssh-server & 
        hdfs namenode & 
        sleep 2 && yarn resourcemanager &
        sleep 3 && mapred historyserver & 
        wait
      "
    networks:
      hadoopnet:
        aliases: 
          - namenode
          - resourcemanager
          - historyserver
          - sparkmaster
    ports:
      - target: 22
        published: 2222
        protocol: tcp
        mode: host
    env_file:
      - ./config-h3-nodes-swarm.env
    environment:
      ENSURE_NAMENODE_DIR: "/tmp/hadoop-hadoop/dfs/name"
      SSH_USER_PASSWORD: x
    deploy:
      endpoint_mode: dnsrr
      mode: global
      placement:
        constraints:
          - node.labels.hadooprole == master

  node01: &workernoderef
    image: remstef/hadoop3
    hostname: node01
    command: >-
      /bin/bash -c "
        hdfs datanode &
        sleep 1 && yarn nodemanager & 
        wait
      "
    env_file:
      - ./config-h3-nodes-swarm.env
    environment:
      - SLEEP_SECONDS=5
      - WAITFOR=headnode:8020
    networks:
      hadoopnet:
        aliases: 
          - datanode01
          - nodemanager01
    deploy:
      endpoint_mode: dnsrr
      mode: global
      placement:
        constraints:
          - node.labels.hadooprole == worker1

  node02:
    <<: *workernoderef
    hostname: node02
    networks:
      hadoopnet:
        aliases: 
          - datanode02
          - nodemanager02
    deploy:
      endpoint_mode: dnsrr
      mode: global
      placement:
        constraints:
          - node.labels.hadooprole == worker2

  node03:
    <<: *workernoderef
    hostname: node03
    networks:
      hadoopnet:
        aliases: 
          - datanode03
          - nodemanager03
    deploy:
      endpoint_mode: dnsrr
      mode: global
      placement:
        constraints:
          - node.labels.hadooprole == worker3
